{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import npz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# from godunov_vis_tools import * \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "from tools_torch import *\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.load('../datasets/controlled_boundary_wavelet_20000\\data_wavelet_boundary_20000_combined.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "branch_coords = data['branch_coords']\n",
    "branch_values = data['branch_values']\n",
    "output_sensor_coords = data['output_sensor_coords']\n",
    "output_sensor_values = data['output_sensor_values']\n",
    "rho = data['rho']\n",
    "# v = data['v']\n",
    "x = data['x']\n",
    "tt = data['t']\n",
    "Nx = data['Nx']\n",
    "Nt = data['Nt']\n",
    "Xmax = data['Xmax']\n",
    "Tmax = data['Tmax']\n",
    "P = data['P']\n",
    "N = data['N']\n",
    "\n",
    "\n",
    "print(f\"branch_coords.shape = {branch_coords.shape}, branch_values.shape = {branch_values.shape}, output_sensor_coords.shape = {output_sensor_coords.shape}, \")\n",
    "print(f\"output_sensor_values.shape = {output_sensor_values.shape}, rho.shape = {rho.shape}, x.shape = {x.shape}, t.shape = {tt.shape}\")\n",
    "print(f\"Nx = {Nx}, Nt = {Nt}, Xmax = {Xmax}, Tmax = {Tmax}, P = {P}, N = {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidon_model import VIDON, FDLearner\n",
    "p = 400\n",
    "model = VIDON(p=p, num_heads=4, d_branch_input=2, d_v=1, use_linear_decoder=False, UQ=True).to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_irregular_boundary_UQTrue_random_sampling_11_nov_p400_extUQ3.pth\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_shape_branch(coords, values, target_shape_boundary, target_shape_probe):\n",
    "    # Separate boundary and probe data based on ID in coords\n",
    "    boundary_data_coords = coords[coords[:, 2] == -1]\n",
    "    probe_data_coords = coords[coords[:, 2] != -1]\n",
    "    \n",
    "    boundary_data_values = values[coords[:, 2] == -1]\n",
    "    probe_data_values = values[coords[:, 2] != -1]\n",
    "    \n",
    "    # Truncate or pad boundary data to target_shape_boundary\n",
    "    filtered_boundary_coords = boundary_data_coords[:target_shape_boundary]\n",
    "    filtered_boundary_values = boundary_data_values[:target_shape_boundary]\n",
    "    \n",
    "    # Truncate or pad probe data to target_shape_probe\n",
    "    if probe_data_coords.shape[0] < target_shape_probe:\n",
    "        # Pad if probe data is smaller than target_shape_probe\n",
    "        pad_size = target_shape_probe - probe_data_coords.shape[0]\n",
    "        \n",
    "        # Generate random values from existing probe data\n",
    "        random_indices = np.random.choice(probe_data_coords.shape[0], size=pad_size)\n",
    "        random_coords = probe_data_coords[random_indices]\n",
    "        random_values = probe_data_values[random_indices]\n",
    "        \n",
    "        # Concatenate original and random padded data\n",
    "        filtered_probe_coords = np.concatenate([probe_data_coords, random_coords], axis=0)\n",
    "        filtered_probe_values = np.concatenate([probe_data_values, random_values], axis=0)\n",
    "    else:\n",
    "        # Truncate if probe data is larger than target_shape_probe\n",
    "        filtered_probe_coords = probe_data_coords[:target_shape_probe]\n",
    "        filtered_probe_values = probe_data_values[:target_shape_probe]\n",
    "\n",
    "    # Combine boundary and probe data back together\n",
    "    filtered_coords = np.vstack([filtered_boundary_coords, filtered_probe_coords])\n",
    "    filtered_values = np.vstack([filtered_boundary_values, filtered_probe_values])\n",
    "\n",
    "    return filtered_coords, filtered_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_sub = tt[:,::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "n_probes = 8  # Replace with the desired number of unique IDs to sample\n",
    "max_id = 0\n",
    "min_id = 1e6\n",
    "\n",
    "T_PRED = 8\n",
    "T_PAST = 2\n",
    "\n",
    "def create_val_loader_for_shifted_t(t_start, branch_coords, branch_values, output_sensor_coords, output_sensor_values):\n",
    "    branch_coords_filtered_list, branch_values_filtered_list = [], []\n",
    "    output_sensor_coords_filtered_list, output_sensor_values_filtered_list = [], []\n",
    "    t_starts = []\n",
    "\n",
    "    tt_sub = tt[:,::5]\n",
    "    tt_starts = tt_sub[tt_sub <= Tmax - T_PRED - T_PAST]\n",
    "\n",
    "\n",
    "    # for every scenario\n",
    "    for idx in range(len(branch_coords)):\n",
    "\n",
    "        t = t_start + T_PAST\n",
    "        t_starts.append(t_start)\n",
    "\n",
    "\n",
    "        # Filter out coordinates where ID is -2\n",
    "        coords_probes_boundary = branch_coords[idx][branch_coords[idx, :, 2] != -2]\n",
    "\n",
    "        # Further filter based on t_max for sampling purposes\n",
    "        coords_in_horizon = coords_probes_boundary[(coords_probes_boundary[:, 1] <= t + T_PRED) & (coords_probes_boundary[:, 1] >= t - T_PAST)]\n",
    "\n",
    "        # # Get all unique IDs except -1 and -2\n",
    "        unique_ids = np.unique(coords_in_horizon[:, 2])\n",
    "        unique_ids = unique_ids[(unique_ids != -1) & (unique_ids != -2)]\n",
    "\n",
    "        # Sample IDs with even spacing\n",
    "        sampled_ids = unique_ids[::max(1, len(unique_ids) // n_probes)]  # Evenly spaced selection of IDs\n",
    "\n",
    "        # Create a mask for branch_coords to keep points with the sampled IDs and -1\n",
    "        mask_sampled_ids = np.isin(branch_coords[idx][:, 2], sampled_ids) & (branch_coords[idx][:, 1] <= t) & (branch_coords[idx][:, 1] >= t - T_PAST)\n",
    "        \n",
    "        # Mask for ID == -1 entries\n",
    "        mask_boundary = branch_coords[idx][:, 2] == -1\n",
    "        \n",
    "        # # Remove ID == -1 entries if x location is below 5 or above t_max_boundary\n",
    "        mask_id_neg1_x_above_5 = mask_boundary & (branch_coords[idx][:, 0] >= 4) & (branch_coords[idx][:, 1] <= t + T_PRED) & (branch_coords[idx][:, 1] >= t - T_PAST)\n",
    "        \n",
    "        # Get indices of ID == -1 and evenly sample half the points\n",
    "        neg1_indices = np.where(mask_id_neg1_x_above_5)[0]\n",
    "        # half_neg1_indices = np.random.choice(neg1_indices, size=len(neg1_indices) // 2, replace=False) if len(neg1_indices) > 0 else []\n",
    "        half_neg1_indices = np.random.choice(neg1_indices, size=len(neg1_indices), replace=False) if len(neg1_indices) > 0 else []\n",
    "\n",
    "        # Create a mask for the sampled half of ID == -1\n",
    "        mask_half_neg1 = np.zeros(mask_id_neg1_x_above_5.shape, dtype=bool)\n",
    "        mask_half_neg1[half_neg1_indices] = True\n",
    "        \n",
    "        # Combine the two masks to filter both branch_coords and branch_values\n",
    "        final_mask = mask_sampled_ids | mask_half_neg1\n",
    "\n",
    "        # Apply the final mask to both branch_coords and branch_values\n",
    "        filtered_coords = branch_coords[idx][final_mask]\n",
    "        filtered_values = branch_values[idx][final_mask]\n",
    "\n",
    "        # shift to t = 0\n",
    "        filtered_coords[:, 1] -= t_start\n",
    "\n",
    "        # # Append the filtered coordinates and values to the list\n",
    "        branch_coords_filtered_list.append(filtered_coords)\n",
    "        branch_values_filtered_list.append(filtered_values)\n",
    "\n",
    "        # keep trunk_coords where t is in horizon\n",
    "        output_sensor_coords_filtered = output_sensor_coords[idx][(output_sensor_coords[idx][:, 1] <= t + T_PRED) & (output_sensor_coords[idx][:, 1] >= t - T_PAST)]\n",
    "        output_sensor_values_filtered = output_sensor_values[idx][(output_sensor_coords[idx][:, 1] <= t + T_PRED) & (output_sensor_coords[idx][:, 1] >= t - T_PAST)]\n",
    "\n",
    "        # shift to t = 0\n",
    "        output_sensor_coords_filtered[:, 1] -= t_start\n",
    "\n",
    "        # Append the filtered coordinates and values to the list\n",
    "        output_sensor_coords_filtered_list.append(output_sensor_coords_filtered)\n",
    "        output_sensor_values_filtered_list.append(output_sensor_values_filtered)\n",
    "\n",
    "\n",
    "    # pad to shape of max_id\n",
    "    filtered_coords_values = [\n",
    "        pad_to_shape_branch(coords, values, target_shape_boundary=188, target_shape_probe=200) \n",
    "        for coords, values in zip(branch_coords_filtered_list, branch_values_filtered_list)\n",
    "    ]\n",
    "\n",
    "    # Split into separate lists if needed\n",
    "    filtered_coords_padded = np.array([item[0] for item in filtered_coords_values])\n",
    "    filtered_values_padded = np.array([item[1] for item in filtered_coords_values])\n",
    "\n",
    "    m_min = min(arr.shape[0] for arr in output_sensor_coords_filtered_list)\n",
    "\n",
    "    # Stack the arrays, trimming each to m_min rows\n",
    "    filtered_output_coords_padded = np.stack([arr[-m_min:] for arr in output_sensor_coords_filtered_list])\n",
    "    filtered_output_values_padded = np.stack([arr[-m_min:] for arr in output_sensor_values_filtered_list])\n",
    "\n",
    "    # make dataloader\n",
    "    branch_coords = torch.tensor(filtered_coords_padded.astype(np.float16))\n",
    "    branch_values = torch.tensor(filtered_values_padded.astype(np.float16))\n",
    "    output_sensor_coords = torch.tensor(filtered_output_coords_padded.astype(np.float16))\n",
    "    output_sensor_values = torch.tensor(filtered_output_values_padded.astype(np.float16))\n",
    "    # rho = torch.tensor(rho.astype(np.float16))\n",
    "    t_starts = torch.tensor(np.array(t_starts))\n",
    "\n",
    "    val_dataset  = DeepONetDatasetTrain(branch_coords[:,:,:], branch_values[:,:,0:1], output_sensor_coords, output_sensor_values, device=device)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_possible = sum(sum([tt_sub[0] < 15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Add timing for val loader creation and validation\n",
    "skip_factor = 1\n",
    "mse_per_timestep = []\n",
    "mae_per_timestep = []\n",
    "\n",
    "for _, timestep in tqdm(\n",
    "    enumerate(tt_sub[0][tt_sub[0] < 15][::skip_factor]), \n",
    "    total=int(shifts_possible / skip_factor), \n",
    "    desc=\"Processing timesteps\"\n",
    "):\n",
    "    tqdm.write(f\"Current Timestep: {timestep:.2f}\")  # Print current timestep dynamically\n",
    "\n",
    "    # Timer for validation loader creation\n",
    "    start_val_loader = time.time()\n",
    "    val_loader = create_val_loader_for_shifted_t(\n",
    "        timestep, branch_coords, branch_values, output_sensor_coords, output_sensor_values\n",
    "    )\n",
    "    val_loader_time = time.time() - start_val_loader\n",
    "    # tqdm.write(f\"Validation Loader Creation Time: {val_loader_time:.4f} seconds\")\n",
    "\n",
    "    # Validate model on validation set\n",
    "    model.eval()\n",
    "\n",
    "    losses = []  # Initialize for tracking loss\n",
    "    mae_errors = []  # Initialize for tracking MAE\n",
    "\n",
    "    # Timer for torch.no_grad section\n",
    "    start_no_grad = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (branch_coords_, branch_values_, trunk_coords_, trunk_values_) in enumerate(val_loader):\n",
    "            branch_coords_ = branch_coords_.to(device)\n",
    "            branch_values_ = branch_values_.to(device)\n",
    "            trunk_coords_ = trunk_coords_.to(device)\n",
    "            trunk_values_ = trunk_values_.to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                rho_pred, sigma = model(branch_coords_[:, :, :2], branch_values_, trunk_coords_)\n",
    "                loss = criterion(rho_pred, trunk_values_)\n",
    "                mae = F.l1_loss(rho_pred, trunk_values_, reduction='mean')  # Calculate MAE\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            mae_errors.append(mae.item())\n",
    "    no_grad_time = time.time() - start_no_grad\n",
    "    # tqdm.write(f\"Torch.no_grad Section Time: {no_grad_time:.4f} seconds\")\n",
    "\n",
    "    # Calculate mean of the losses and MAE errors\n",
    "    validation_loss = np.mean(losses)\n",
    "    mean_absolute_error = np.mean(mae_errors)\n",
    "\n",
    "    mse_per_timestep.append(validation_loss)\n",
    "    mae_per_timestep.append(mean_absolute_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_per_timestep)\n",
    "print(np.mean(mse_per_timestep))\n",
    "# plt.plot(mse_per_timestep_receding)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_time_window(t_start, branch_coords, branch_values, output_sensor_coords, output_sensor_values, idx):\n",
    "    # randomly sample a shift between 0 and T_max - T_PRED - T_PAST\n",
    "    t = t_start + T_PAST\n",
    "\n",
    "    # Filter out coordinates where ID is -2\n",
    "    coords_probes_boundary = branch_coords[idx][branch_coords[idx, :, 2] != -2]\n",
    "\n",
    "    # Further filter based on t_max for sampling purposes\n",
    "    coords_in_horizon = coords_probes_boundary[(coords_probes_boundary[:, 1] <= t + T_PRED) & (coords_probes_boundary[:, 1] >= t - T_PAST)]\n",
    "\n",
    "    # # Get all unique IDs except -1 and -2\n",
    "    unique_ids = np.unique(coords_in_horizon[:, 2])\n",
    "    unique_ids = unique_ids[(unique_ids != -1) & (unique_ids != -2)]\n",
    "\n",
    "    # Sample IDs with even spacing\n",
    "    sampled_ids = unique_ids[::max(1, len(unique_ids) // n_probes)]  # Evenly spaced selection of IDs\n",
    "    print(sampled_ids)\n",
    "\n",
    "    # Create a mask for branch_coords to keep points with the sampled IDs and -1\n",
    "    mask_sampled_ids = np.isin(branch_coords[idx][:, 2], sampled_ids) & (branch_coords[idx][:, 1] <= t) & (branch_coords[idx][:, 1] >= t - T_PAST)\n",
    "    \n",
    "    # Mask for ID == -1 entries\n",
    "    mask_boundary = branch_coords[idx][:, 2] == -1\n",
    "    \n",
    "    # # Remove ID == -1 entries if x location is below 5 or above t_max_boundary\n",
    "    mask_id_neg1_x_above_5 = mask_boundary & (branch_coords[idx][:, 0] >= 4) & (branch_coords[idx][:, 1] <= t + T_PRED) & (branch_coords[idx][:, 1] >= t - T_PAST)\n",
    "    \n",
    "    # Get indices of ID == -1 and evenly sample half the points\n",
    "    neg1_indices = np.where(mask_id_neg1_x_above_5)[0]\n",
    "    # half_neg1_indices = np.random.choice(neg1_indices, size=len(neg1_indices) // 2, replace=False) if len(neg1_indices) > 0 else []\n",
    "    half_neg1_indices = np.random.choice(neg1_indices, size=len(neg1_indices), replace=False) if len(neg1_indices) > 0 else []\n",
    "\n",
    "    # Create a mask for the sampled half of ID == -1\n",
    "    mask_half_neg1 = np.zeros(mask_id_neg1_x_above_5.shape, dtype=bool)\n",
    "    mask_half_neg1[half_neg1_indices] = True\n",
    "    \n",
    "    # Combine the two masks to filter both branch_coords and branch_values\n",
    "    final_mask = mask_sampled_ids | mask_half_neg1\n",
    "\n",
    "    # Apply the final mask to both branch_coords and branch_values\n",
    "    filtered_coords = branch_coords[idx][final_mask]\n",
    "    filtered_values = branch_values[idx][final_mask]\n",
    "\n",
    "    output_sensor_coords_filtered = output_sensor_coords[idx][(output_sensor_coords[idx][:, 1] <= t + T_PRED) & (output_sensor_coords[idx][:, 1] >= t - T_PAST)]\n",
    "    output_sensor_values_filtered = output_sensor_values[idx][(output_sensor_coords[idx][:, 1] <= t + T_PRED) & (output_sensor_coords[idx][:, 1] >= t - T_PAST)]\n",
    "\n",
    "    return filtered_coords, filtered_values, output_sensor_coords_filtered, output_sensor_values_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
