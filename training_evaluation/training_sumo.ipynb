{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import importlib\n",
    "import tools_torch\n",
    "importlib.reload(tools_torch)\n",
    "from tools_torch import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../datasets/sumo/sumo_idm_dataset_tpast2_tpred8.npz\")\n",
    "\n",
    "# Extract the data\n",
    "branch_coords = torch.tensor(data['branch_coords'])\n",
    "branch_values = torch.tensor(data['branch_values'])\n",
    "output_sensor_coords = torch.tensor(data['output_sensor_coords'])\n",
    "output_sensor_values = torch.tensor(data['output_sensor_values'])\n",
    "rho = torch.tensor(data['rho'])\n",
    "\n",
    "Nx = data['Nx'].item()\n",
    "Nt = data['Nt'].item()\n",
    "Xmax = data['Xmax'].item()\n",
    "Tmax = data['Tmax'].item()\n",
    "N = data['N'].item()\n",
    "t_starts = torch.tensor(data['t_starts'])\n",
    "t_pred = data['t_pred'].item()\n",
    "t_past = data['t_past'].item()\n",
    "t = np.linspace(0, Tmax, Nt)\n",
    "\n",
    "print(f\"Nx = {Nx}, Nt = {Nt}, Xmax = {Xmax}, Tmax = {Tmax}, N = {N}\")\n",
    "print(f\"branch_coords.shape = {branch_coords.shape}, branch_values.shape = {branch_values.shape}, output_sensor_coords.shape = {output_sensor_coords.shape}, \")\n",
    "print(f\"output_sensor_values.shape = {output_sensor_values.shape}\")\n",
    "print(f\"t_starts = {t_starts.shape}\")\n",
    "print(f\", rho.shape = {rho.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the validation percentage\n",
    "validation_percentage = 0.2  # validation\n",
    "print((1-validation_percentage)*N)\n",
    "\n",
    "branch_coords_train, branch_coords_val, branch_values_train, branch_values_val, \\\n",
    "output_sensor_coords_train, output_sensor_coords_val, output_sensor_values_train, output_sensor_values_val, rho_train, rho_val, \\\n",
    "t_starts_train, t_starts_val = train_test_split(\n",
    "    branch_coords, branch_values, output_sensor_coords, output_sensor_values, rho, t_starts,\n",
    "    test_size=validation_percentage, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataset = DeepONetDatasetTrain(branch_coords_train[:,:,:], branch_values_train[:,:,:], output_sensor_coords_train, output_sensor_values_train, device=device) # branch_coord, branch_values, trunk_coords, targets, UU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataset = DeepONetDatasetTrain(branch_coords_val[:,:,:], branch_values_val[:,:,:], output_sensor_coords_val, output_sensor_values_val, device=device)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "xs, us, ys, ss = next(iter(train_loader))\n",
    "print(f\"train shapes\\t xs: {xs.shape}, us: {us.shape}, ys: {ys.shape}, ss: {ss.shape}, device: {xs.device}\")\n",
    "xs, us, ys, ss = next(iter(val_loader))\n",
    "print(f\"val shapes\\t xs: {xs.shape}, us: {us.shape}, ys: {ys.shape}, ss: {ss.shape}, device: {xs.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1\n",
    "plt.scatter(ys[idx,:,1].cpu().numpy(), ys[idx,:,0].cpu().numpy(), c=ss[idx,:].cpu().numpy(), label='output sensors',  cmap='jet', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.scatter(xs[idx,:,1].cpu().numpy(), xs[idx,:,0].cpu().numpy(), c=us[idx,:,0].cpu().numpy(), label='branch sensors', cmap='jet', vmin=0, vmax=1)\n",
    "plt.ylim(0), plt.xlim(0)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "from vidon_model import VIDON, FDLearner\n",
    "p = 400\n",
    "model = VIDON(p=p, num_heads=4, d_branch_input=3, d_v=2, use_linear_decoder=False, UQ=True).to(device)\n",
    "model.to(device)\n",
    "FD = FDLearner(d_FD=50)\n",
    "FD.to(device)\n",
    "\n",
    "# Define the loss function \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    list(model.parameters()) + list(FD.parameters()), \n",
    "    lr=0.001\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=50,\n",
    "    min_lr=1e-7,\n",
    "    cooldown=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "# print number of parameters\n",
    "print(f\"Number of parameters in coord encoder: {sum(p.numel() for p in model.branch.coord_encoder.parameters())}\")\n",
    "print(f\"Number of parameters in value encoder: {sum(p.numel() for p in model.branch.value_encoder.parameters())}\")\n",
    "print(f\"Number of parameters in combiner: {sum(p.numel() for p in model.branch.combiner.parameters())}\")\n",
    "print(f\"Number of parameters in nonlinear decoder: {sum(p.numel() for p in model.nonlinear_decoder.parameters())}\")\n",
    "print(f\"Number of parameters in trunk: {sum(p.numel() for p in model.trunk.parameters())}\")\n",
    "print(f\"Number of parameters in FD:  \\t{sum(p.numel() for p in FD.parameters())}\")\n",
    "print(f\"Number of parameters in model: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # freeze except for UQ layers\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# model.bias.requires_grad = False\n",
    "# # # Unfreeze parameters for specific components\n",
    "# for param in model.nonlinear_decoder_sigm.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.branch.combiner_sigm.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# for param in model.trunk.sigm_trunk.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# model=model.float()\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_norms = []\n",
    "loss_list = []\n",
    "loss_ic_list = []\n",
    "val_loss_list = []\n",
    "val_loss_mse_list = []\n",
    "phys_losses_list = []\n",
    "sigma_list = []\n",
    "v_loss_list = []\n",
    "lrs = []\n",
    "lambdas = []\n",
    "gammas = []\n",
    "val_loss = 0\n",
    "lowest_val_loss = 0.1\n",
    "\n",
    "T_PAST = 2\n",
    "T_PRED = 8\n",
    "\n",
    "previous_epoch_loss_data = float('inf')\n",
    "previous_epoch_loss_lwr = float('inf')\n",
    "\n",
    "# Training loop \n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    losses = []\n",
    "    v_losses = []\n",
    "    losses_ic = []\n",
    "    sigmas = []\n",
    "\n",
    "    T_START = None\n",
    "    \n",
    "    for branch_coord, branch_values, trunk_coords, targets in train_loader:\n",
    "\n",
    "        # bring to gpu\n",
    "        branch_coord = branch_coord.to(device)\n",
    "        branch_values = branch_values.to(device)\n",
    "        trunk_coords = trunk_coords.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        filtered_trunk_coords, filtered_targets = trunk_coords, targets\n",
    "        filtered_branch_coords, filtered_branch_values = branch_coord, branch_values\n",
    "\n",
    "        # select random parts\n",
    "        sampled_trunk_coords, sampled_targets = sample_trunk_inputs(filtered_trunk_coords, filtered_targets)\n",
    "        sampled_branch_coord, sampled_branch_values = sample_branch_inputs_keep_boundary(filtered_branch_coords, filtered_branch_values, min_to_keep=0.8)\n",
    "\n",
    "        \n",
    "        # sampled_trunk_coords, sampled_targets = trunk_coords, targets\n",
    "        # sampled_branch_coord, sampled_branch_values = branch_coord, branch_values\n",
    "\n",
    "        sampled_trunk_coords = sampled_trunk_coords.clone().detach().requires_grad_(True)\n",
    "\n",
    "    \n",
    "        # forward pass for rho and v with automatic mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "        \n",
    "            outputs_rho, outputs_rho_sigm = model(sampled_branch_coord, sampled_branch_values, sampled_trunk_coords) # at collocation points\n",
    "            outputs_rho_sigm = torch.exp(outputs_rho_sigm) + 1e-8\n",
    "            v_pred_phys = FD(outputs_rho.unsqueeze(-1))\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_first_term = (outputs_rho - sampled_targets) ** 2 / (2 * (outputs_rho_sigm) ** 2)\n",
    "            loss_second_term = 0.5 * torch.log(2 * torch.pi * (outputs_rho_sigm) ** 2)\n",
    "            loss_UQ = torch.mean(loss_first_term + loss_second_term)\n",
    "            mse_loss = criterion(outputs_rho, sampled_targets)\n",
    "            \n",
    "\n",
    "            outputs_ic_rho, outputs_ic_sigm = model(sampled_branch_coord, sampled_branch_values, sampled_branch_coord[:,:,:2]) # at probe points\n",
    "            loss_ic = criterion(outputs_ic_rho, sampled_branch_values[:,:,0].squeeze())\n",
    "\n",
    "            # Create the mask where sampled_branch_coord[:, :, 2] > -1\n",
    "            mask = sampled_branch_coord[:, :, 2] > -1  # Mask to select valid elements\n",
    "\n",
    "            # Apply the mask to outputs and sampled values\n",
    "            outputs_ic_rho_probe = outputs_ic_rho[mask].reshape(-1, 1)  # Reshape to match expected shape\n",
    "            sampled_branch_values_probe = sampled_branch_values[:, :, 1][mask].reshape(-1, 1)  # Mask and reshape to match\n",
    "\n",
    "            # Compute velocity predictions and loss\n",
    "            v_pred_data = FD(outputs_ic_rho_probe)  # Ensure the input shape matches FD's expectations\n",
    "            v_loss = criterion(v_pred_data, sampled_branch_values_probe)\n",
    "\n",
    "            # Total loss\n",
    "            loss = mse_loss + v_loss/1000 + loss_UQ/300\n",
    "\n",
    "            losses.append(mse_loss.item())\n",
    "            v_losses.append(v_loss.item())\n",
    "            sigmas.append(torch.mean(outputs_rho_sigm).item())\n",
    "        \n",
    "        \n",
    "        # Backward pass with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)  # Clip gradient norm\n",
    "        \n",
    "        # Update weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    loss_list.append(np.mean(losses))\n",
    "    loss_ic_list.append(np.mean(losses_ic))\n",
    "    sigma_list.append(np.mean(sigmas))\n",
    "\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # validation loss\n",
    "    if epoch % 1 == 0: # TODO: save on smallest validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_losses_mse = []\n",
    "            T_START = 0 \n",
    "\n",
    "            # Timing evaluation\n",
    "            for branch_coord, branch_values, trunk_coords, targets in val_loader:\n",
    "                branch_coord = branch_coord.to(device)\n",
    "                branch_values = branch_values.to(device)\n",
    "                trunk_coords = trunk_coords.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                \n",
    "                with autocast():\n",
    "                    if model.UQ:\n",
    "                        outputs_rho, outputs_rho_sigm = model(filtered_branch_coords[:,:,:], filtered_branch_values, filtered_trunk_coords)\n",
    "                        outputs_rho_sigm = torch.exp(outputs_rho_sigm) + 1e-8\n",
    "\n",
    "                        # Compute the loss\n",
    "                        loss_first_term = (outputs_rho - filtered_targets) ** 2 / (2 * outputs_rho_sigm)\n",
    "                        loss_second_term = 0.5 * torch.log(2 * torch.pi * outputs_rho_sigm)\n",
    "                        val_loss = torch.mean(loss_first_term + loss_second_term).item()\n",
    "                        val_loss_mse = criterion(outputs_rho, filtered_targets).item()\n",
    "\n",
    "                    else:\n",
    "                        outputs = model(filtered_branch_coords[:,:,:2], filtered_branch_values, filtered_trunk_coords)\n",
    "                        # outputs = model(torch.cat((branch_coord, torch.zeros_like(branch_coord)), dim=-1), branch_values, trunk_coords)\n",
    "                        val_loss = criterion(outputs, filtered_targets).item()\n",
    "                        val_loss_mse = val_loss\n",
    "                    \n",
    "                    val_losses.append(val_loss)\n",
    "                    val_losses_mse.append(val_loss_mse)\n",
    "        \n",
    "        val_loss_list.append(np.mean(val_losses))\n",
    "        val_loss_mse_list.append(np.mean(val_losses_mse))\n",
    "        v_loss_list.append(np.mean(v_losses))\n",
    "        \n",
    "    \n",
    "    # pbar.set_postfix({'Loss': loss.item(), 'Val Loss': val_loss, 'Val Loss MSE': np.mean(val_losses_mse), \"learning_rate\": lrs[-1]})\n",
    "    pbar.set_postfix({'Loss': np.mean(losses), 'Val Loss': val_loss, 'Val Loss MSE': np.mean(val_losses_mse), \"FD loss\": np.mean(v_losses)})\n",
    "\n",
    "    # Update previous epoch loss values\n",
    "    previous_epoch_loss_data = np.mean(losses)\n",
    "\n",
    "\n",
    "    if np.mean(val_losses_mse) < lowest_val_loss:\n",
    "        lowest_val_loss = np.mean(val_losses_mse)\n",
    "        print(f\"Saving model with lowest validation loss: {lowest_val_loss}\")\n",
    "        torch.save(model.state_dict(), f'model_sumo_19_11_past2_pred8_2_8000samples_UQ.pth')\n",
    "\n",
    "\n",
    "    scheduler.step(np.mean(val_losses_mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    plt.scatter(sampled_branch_coord[i,:,1].cpu().detach().numpy(), sampled_branch_coord[i,:,0].cpu().detach().numpy(), c=sampled_branch_values[i,:,0].cpu().detach().numpy(), label='output sensors',  cmap='jet', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('loss_data_20_11_sumo_past2_pred8_2_8000samples.npz', loss_list=loss_list, val_loss_mse_list=val_loss_mse_list, lambdas=lambdas, phys_losses_list=phys_losses_list)\n",
    "\n",
    "# Plot the losses\n",
    "plt.semilogy(loss_list, label='Training Loss')  # Plot the training loss\n",
    "plt.semilogy(val_loss_mse_list, label='Validation Loss')  # Plot the validation loss\n",
    "# plt.semilogy(np.array(sigma_list), label='Sigma')  # Plot the validation loss\n",
    "# plt.semilogy(lambdas, label='Gamma')  # Plot the validation loss\n",
    "# plt.semilogy(phys_losses_list, label='Physics Loss')  # Plot the validation loss\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "# plt.yscale('linear')\n",
    "plt.show()\n",
    "\n",
    "# Print the minimum validation loss for reference\n",
    "print(f\"Minimum Validation Loss: {min(val_loss_mse_list)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
